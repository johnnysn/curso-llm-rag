{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.llms import Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'properties': {'tema': {'title': 'Tema', 'type': 'string'}},\n",
       " 'required': ['tema'],\n",
       " 'title': 'PromptInput',\n",
       " 'type': 'object'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', 'Você é um humorista e faz piadas conforme solicitado pelo usuário.'),\n",
    "    ('human', 'Faça uma piada sobre o tema: {tema}')\n",
    "])\n",
    "model = Ollama(model='llama3.1')\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "chain.input_schema.schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Que espaço para rir!\\n\\nPor que os astronautas sempre vão ao psicólogo? Porque estão \"outros\" em crise! (saudades do espaço são real!)\\n\\nMas sério, por que as missões espaciais sempre levam uma quantidade impressionante de... você adivinhou? Cerveja! É para manter os nervos estabilizados quando você está fazendo um pouquinho de \"gravidade\" no universo!\\n\\nE sabe o que é melhor? Quando eles voltam da missão, as pessoas perguntam: \"Como foi?\" E eles respondem... \"Foi um \\'blasta\\'!\" (só quem foi ao espaço entende!)\\n\\nO que você acha dessa piada? Quer mais?'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'tema': 'astronautas'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Por que os astronautas se sentem sempre desconfortáveis? Porque estão sempre fora do seu elemento!\n",
      "\n",
      "(Por favor, diga que gostou ou quiser mais!)\n"
     ]
    }
   ],
   "source": [
    "stream = chain.stream({'tema': 'astronautas'})\n",
    "\n",
    "for chunk in stream:\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'properties': {'topic': {'title': 'Topic', 'type': 'string'}}, 'required': ['topic'], 'title': 'PromptInput', 'type': 'object'}\n",
      "Here's one:\n",
      "\n",
      "Why did the astronaut break up with his girlfriend before going to Mars?\n",
      "\n",
      "Because he needed space! (get it?)\n"
     ]
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate([\n",
    "    ('system', 'You are a comedian and make jokes as requested by the user.'),\n",
    "    ('human', 'Tell me a joke about the topic: {topic}')\n",
    "])\n",
    "\n",
    "model = Ollama(model='llama3.1', temperature=0.1)\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "print(chain.input_schema.schema())\n",
    "\n",
    "topic = input('Enter a topic for the question: ')\n",
    "\n",
    "stream = chain.stream({'topic': topic})\n",
    "for chunk in stream:\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seja amável com as pessoas o tempo todo, Jeremy.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_translate = PromptTemplate.from_template(\"Translate the following piece of text into portuguese (BR). Provide the translation ONLY. \\n {text}\")\n",
    "\n",
    "# model_translate = Ollama(model=\"llama3.1\", temperature = 0.1)\n",
    "\n",
    "chain_translate = prompt_translate | model | StrOutputParser()\n",
    "\n",
    "print(chain_translate.invoke({'text': 'Be nice to people all the time, Jeremy'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'StrOutputParserOutput', 'type': 'string'}\n",
      "{'properties': {'text': {'title': 'Text', 'type': 'string'}}, 'required': ['text'], 'title': 'PromptInput', 'type': 'object'}\n"
     ]
    }
   ],
   "source": [
    "print(chain.output_schema.schema())\n",
    "\n",
    "print(chain_translate.input_schema.schema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Por que o astronauta se separou da namorada antes de ir para Marte?\n",
      "\n",
      "Porque ele precisava de espaço! (entendeu?)\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "composed_chain = chain | {'text': RunnablePassthrough()} | chain_translate\n",
    "\n",
    "print(composed_chain.invoke({'topic': 'astronaut'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Por que o astronauta se separou da namorada antes de ir para Marte?\n",
      "\n",
      "Porque ele precisava de espaço! (entendeu?)\n"
     ]
    }
   ],
   "source": [
    "prompt_input = PromptTemplate.from_template(\"Translate the following portuguese topic into english. Provide ONLY the translation. \\n {topic}\")\n",
    "\n",
    "composed_chain = (\n",
    "    prompt_input \n",
    "    | model \n",
    "    | StrOutputParser()\n",
    "    | {'topic': RunnablePassthrough()}\n",
    "    | chain\n",
    "    | {'text': RunnablePassthrough()}\n",
    "    | chain_translate\n",
    ")\n",
    "\n",
    "stream = composed_chain.stream({'topic', 'Astronauta'})\n",
    "for chunk in stream:\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
