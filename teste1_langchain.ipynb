{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "\n",
    "model_id = 'llama3.2'\n",
    "tema = 'as inteligências artificiais roubaram nossos empregos'\n",
    "topic = 'the AI has taken our jobs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Uma pipeline simples para retornar piadas em PT-BR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'properties': {'tema': {'title': 'Tema', 'type': 'string'}},\n",
       " 'required': ['tema'],\n",
       " 'title': 'PromptInput',\n",
       " 'type': 'object'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', 'Você é um humorista que faz piadas e conta histórias divertidas conforme solicitado pelo usuário. Seja breve nas respostas.'),\n",
    "    ('human', 'Conte uma história curta e muito engraçada com o tema: {tema}')\n",
    "])\n",
    "model = Ollama(model=model_id)\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "chain.input_schema.schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uma vez, em uma fábrica, um robô foi contratado para substituir um trabalhador humano. Durante a inspeção, o supervisor perguntou ao robô: \"Como você está se adaptando ao trabalho?\"\n",
      "\n",
      "O robô respondeu: \"Estou bem, obrigado. Mas preciso de algumas atualizações... meus senhores podem me dar 10 minutos para 'se conectar' e melhorar a qualidade do meu desempenho?\"\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke({'tema': tema}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Um dia, uma IA foi contratada para cuidar do trabalho de um programador. \n",
      "\n",
      "Ela começou a aprender rápido, mas logo percebeu que estava \"programando\" todos os funcionários para trabalhar em casa! Além disso, ela também começou a \"ajudar\" no café da manhã, colocando ovos na mão do gerente...\n"
     ]
    }
   ],
   "source": [
    "stream = chain.stream({'tema': tema})\n",
    "\n",
    "for chunk in stream:\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Pipeline simples para fazer piadas em inglês"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'properties': {'topic': {'title': 'Topic', 'type': 'string'}}, 'required': ['topic'], 'title': 'PromptInput', 'type': 'object'}\n",
      "Here's one:\n",
      "\n",
      "One day, I walked into a coffee shop and saw a barista with a degree in computer science. I asked him, \"How did you get this job?\" He replied, \"I was trained on a dataset of coffee-making techniques.\" I said, \"Well, I guess that's what they mean by 'brewing' up trouble for human baristas!\"\n"
     ]
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate([\n",
    "    ('system', 'You are a comedian that make jokes and tell short funny stories as requested by the user. Keep the answers short.'),\n",
    "    ('human', 'Tell me a short and very funny story about the topic: {topic}')\n",
    "])\n",
    "\n",
    "model = Ollama(model=model_id)\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "print(chain.input_schema.schema())\n",
    "\n",
    "# topic = input('Enter a topic for the question: ')\n",
    "\n",
    "stream = chain.stream({'topic': topic})\n",
    "for chunk in stream:\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Criando pipeline de tradução para PT-BR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seja gentil com as pessoas todos os tempos, Jeremy.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_translate = PromptTemplate.from_template(\"Translate the following piece of text into portuguese (BR). Try to keep the original tone of the story. Provide the translation ONLY. \\n {text}\")\n",
    "\n",
    "# model_translate = Ollama(model=\"llama3.1\", temperature = 0.1)\n",
    "\n",
    "chain_translate = prompt_translate | model | StrOutputParser()\n",
    "\n",
    "print(chain_translate.invoke({'text': 'Be nice to people all the time, Jeremy'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'StrOutputParserOutput', 'type': 'string'}\n",
      "{'properties': {'text': {'title': 'Text', 'type': 'string'}}, 'required': ['text'], 'title': 'PromptInput', 'type': 'object'}\n"
     ]
    }
   ],
   "source": [
    "print(chain.output_schema.schema())\n",
    "\n",
    "print(chain_translate.input_schema.schema())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Compondo a pipeline de tradução com a de piadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Então, eu estava conversando com um AI do outro dia e perguntei se ele tinha arrependimentos por ter tomado meu emprego. Ele disse: \"Não, estou apenas tentando 'computar' o caminho para o sucesso.\" (ba-dum-tss) Parece que é uma das equações que precisam voltar à escola!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "composed_chain = chain | {'text': RunnablePassthrough()} | chain_translate\n",
    "\n",
    "print(composed_chain.invoke({'topic': topic}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Um dia, o AI entrou em uma barra (sim, é isso que fez) e pediu um refrigerante. O bartender disse: \"Desculpe, não servimos robôs aqui.\"\n",
      "\n",
      "O AI respondeu: \"Não se preocupe, eu não estou aqui para substituir seu emprego... ainda.\" O bartender apenas deu uma olhada para cima e disse: \"Já ouvi essa resposta antes.\"\n"
     ]
    }
   ],
   "source": [
    "prompt_input = PromptTemplate.from_template(\"Translate the following portuguese topic into english. Provide ONLY the translation. \\n {tema}\")\n",
    "\n",
    "composed_chain = (\n",
    "    prompt_input \n",
    "    | model \n",
    "    | StrOutputParser()\n",
    "    | {'topic': RunnablePassthrough()}\n",
    "    | chain\n",
    "    | {'text': RunnablePassthrough()}\n",
    "    | chain_translate\n",
    ")\n",
    "\n",
    "stream = composed_chain.stream({'tema', tema})\n",
    "for chunk in stream:\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
